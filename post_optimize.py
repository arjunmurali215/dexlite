"""
Post-optimization script for reconstructed grasps from inference.
This script takes the grasps generated by the CVAE network and optimizes them
using the same optimization procedure as DexGraspNet/grasp_generation/main.py,
but initializes the hand pose from the network output instead of random initialization.

Author: Auto-generated
Description: Post-optimization for CVAE reconstructed grasps
"""

import os
import sys
import argparse
import shutil
import numpy as np
import torch
from tqdm import tqdm
import math
import transforms3d
import glob

# Add DexGraspNet to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'DexGraspNet', 'grasp_generation'))

from utils.hand_model import HandModel
from utils.object_model import ObjectModel
from utils.energy import cal_energy
from utils.optimizer import Annealing
from utils.rot6d import robust_compute_rotation_matrix_from_ortho6d


def euler_to_rot6d(euler_angles):
    """
    Convert Euler angles (xyz) to rot6d representation.
    
    Parameters
    ----------
    euler_angles: tuple or list of 3 floats
        Euler angles in (rx, ry, rz) format
    
    Returns
    -------
    rot6d: (6,) torch.Tensor
        First two columns of rotation matrix flattened
    """
    rot_mat = transforms3d.euler.euler2mat(euler_angles[0], euler_angles[1], euler_angles[2], axes='sxyz')
    rot_mat = torch.tensor(rot_mat, dtype=torch.float)
    # rot6d is the first two columns of rotation matrix transposed
    rot6d = rot_mat.T[:2].reshape(-1)
    return rot6d


def qpos_to_hand_pose(qpos, device):
    """
    Convert qpos dictionary to hand_pose tensor format.
    
    Parameters
    ----------
    qpos: dict
        Dictionary with translation (WRJTx, WRJTy, WRJTz), 
        rotation (WRJRx, WRJRy, WRJRz in Euler angles),
        and joint angles
    device: torch.device
    
    Returns
    -------
    hand_pose: (31,) torch.Tensor
        [translation (3), rot6d (6), joint_angles (22)]
    """
    translation_names = ['WRJTx', 'WRJTy', 'WRJTz']
    rot_names = ['WRJRx', 'WRJRy', 'WRJRz']
    joint_names = [
        'robot0:FFJ3', 'robot0:FFJ2', 'robot0:FFJ1', 'robot0:FFJ0',
        'robot0:MFJ3', 'robot0:MFJ2', 'robot0:MFJ1', 'robot0:MFJ0',
        'robot0:RFJ3', 'robot0:RFJ2', 'robot0:RFJ1', 'robot0:RFJ0',
        'robot0:LFJ4', 'robot0:LFJ3', 'robot0:LFJ2', 'robot0:LFJ1', 'robot0:LFJ0',
        'robot0:THJ4', 'robot0:THJ3', 'robot0:THJ2', 'robot0:THJ1', 'robot0:THJ0'
    ]
    
    # Get translation
    translation = torch.tensor([qpos[name] for name in translation_names], dtype=torch.float, device=device)
    
    # Get rotation (Euler to rot6d)
    euler = [qpos[name] for name in rot_names]
    rot6d = euler_to_rot6d(euler).to(device)
    
    # Get joint angles
    joint_angles = torch.tensor([qpos[name] for name in joint_names], dtype=torch.float, device=device)
    
    # Concatenate
    hand_pose = torch.cat([translation, rot6d, joint_angles])
    return hand_pose


def initialize_multi_object(hand_model, object_model, grasp_data_list, object_codes, n_contact, device):
    """
    Initialize hand model from inference results for multiple objects.
    
    Parameters
    ----------
    hand_model: HandModel
    object_model: ObjectModel
    grasp_data_list: list of list of dict
        List of grasp data for each object (each is a list of grasp dicts)
    object_codes: list of str
        Object codes corresponding to each grasp_data entry
    n_contact: int
        Number of contact points
    device: torch.device
    
    Returns
    -------
    hand_pose: torch.Tensor
    contact_point_indices: torch.Tensor
    grasp_to_object_idx: list of int
        Maps each grasp index to its object index
    """
    # Flatten all grasps and track which object each belongs to
    all_grasps = []
    grasp_to_object_idx = []
    scales_per_object = {code: [] for code in object_codes}
    
    for obj_idx, (obj_code, grasps) in enumerate(zip(object_codes, grasp_data_list)):
        for grasp in grasps:
            all_grasps.append(grasp)
            grasp_to_object_idx.append(obj_idx)
            scales_per_object[obj_code].append(grasp['scale'])
    
    batch_size = len(all_grasps)
    
    # Convert all grasps to hand_pose format
    hand_poses = []
    for grasp in all_grasps:
        hand_pose = qpos_to_hand_pose(grasp['qpos'], device)
        hand_poses.append(hand_pose)
    
    hand_pose = torch.stack(hand_poses, dim=0)
    hand_pose.requires_grad_()
    
    # Set the object scales - need to match the structure expected by ObjectModel
    # object_scale_tensor shape: (n_objects, batch_size_each)
    # But we have varying grasps per object, so we need to handle this carefully
    # The object_model was initialized with batch_size_each = max grasps per object
    # We'll override the scale tensor to match actual scales
    n_objects = len(object_codes)
    batch_size_each = object_model.batch_size_each
    
    scale_tensor = torch.zeros((n_objects, batch_size_each), dtype=torch.float, device=device)
    for obj_idx, obj_code in enumerate(object_codes):
        obj_scales = scales_per_object[obj_code]
        for i, s in enumerate(obj_scales):
            scale_tensor[obj_idx, i] = s
    
    object_model.object_scale_tensor = scale_tensor
    
    # Initialize contact point indices randomly
    contact_point_indices = torch.randint(0, hand_model.n_contact_candidates, (batch_size, n_contact), device=device)
    
    hand_model.set_parameters(hand_pose, contact_point_indices)
    
    return hand_pose, contact_point_indices, grasp_to_object_idx


def post_optimize(args):
    """Main post-optimization function."""
    
    # Convert paths to absolute before changing directory
    args.input_dir = os.path.abspath(args.input_dir)
    args.output_dir = os.path.abspath(args.output_dir)
    args.mesh_root = os.path.abspath(args.mesh_root)
    
    # Change to the grasp_generation directory so MJCF paths work correctly
    grasp_gen_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'DexGraspNet', 'grasp_generation')
    os.chdir(grasp_gen_dir)
    
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
    np.seterr(all='raise')
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Running on {device}')
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    print(f"Output directory: {args.output_dir}")
    
    # Get list of all grasp files
    grasp_files = sorted(glob.glob(os.path.join(args.input_dir, '*.npy')))
    print(f"Found {len(grasp_files)} grasp files to process")
    print(f"Processing {args.n_objects} object(s) at a time")
    
    # Initialize hand model paths
    mjcf_path = 'mjcf/shadow_hand_wrist_free.xml'
    mesh_path = 'mjcf/meshes'
    contact_points_path = 'mjcf/contact_points.json'
    penetration_points_path = 'mjcf/penetration_points.json'
    
    # Load all grasp data first
    all_object_data = []
    for grasp_file in grasp_files:
        object_code = os.path.basename(grasp_file).replace('.npy', '')
        grasp_data = np.load(grasp_file, allow_pickle=True)
        if len(grasp_data) > 0:
            all_object_data.append({
                'object_code': object_code,
                'grasp_data': list(grasp_data),
                'grasp_file': grasp_file
            })
    
    print(f"Loaded {len(all_object_data)} objects with grasps")
    
    # Process objects in groups of n_objects
    for group_start in range(0, len(all_object_data), args.n_objects):
        group_end = min(group_start + args.n_objects, len(all_object_data))
        object_group = all_object_data[group_start:group_end]
        
        object_codes = [obj['object_code'] for obj in object_group]
        print(f"\n{'='*70}")
        print(f"Processing object group {group_start//args.n_objects + 1}: {', '.join(object_codes)}")
        print(f"{'='*70}")
        
        # Determine batch_size_each (max grasps per object in this group, capped by args.batch_size)
        max_grasps_per_obj = max(len(obj['grasp_data']) for obj in object_group)
        batch_size_each = min(max_grasps_per_obj, args.batch_size)
        
        # Results storage for each object in this group
        group_results = {obj['object_code']: [] for obj in object_group}
        
        # Process in batches if objects have more grasps than batch_size_each
        batch_idx = 0
        while True:
            # Collect grasps for this batch from each object
            batch_grasp_data = []
            batch_object_codes = []
            any_grasps = False
            
            for obj in object_group:
                start_idx = batch_idx * batch_size_each
                end_idx = min(start_idx + batch_size_each, len(obj['grasp_data']))
                
                if start_idx < len(obj['grasp_data']):
                    grasps_slice = obj['grasp_data'][start_idx:end_idx]
                    # Pad to batch_size_each if needed (will be ignored in results)
                    while len(grasps_slice) < batch_size_each:
                        grasps_slice.append(grasps_slice[-1])  # Duplicate last grasp as padding
                    batch_grasp_data.append(grasps_slice)
                    batch_object_codes.append(obj['object_code'])
                    any_grasps = True
                else:
                    # No more grasps for this object, use dummy data
                    batch_grasp_data.append([obj['grasp_data'][-1]] * batch_size_each)
                    batch_object_codes.append(obj['object_code'])
            
            if not any_grasps:
                break
            
            # Track actual grasp counts for this batch
            actual_counts = []
            for obj in object_group:
                start_idx = batch_idx * batch_size_each
                end_idx = min(start_idx + batch_size_each, len(obj['grasp_data']))
                actual_counts.append(max(0, end_idx - start_idx))
            
            total_grasps = sum(actual_counts)
            if total_grasps == 0:
                break
            
            print(f"  Batch {batch_idx + 1}: {total_grasps} grasps across {len(batch_object_codes)} objects")
            
            # Create hand model
            hand_model = HandModel(
                mjcf_path=mjcf_path,
                mesh_path=mesh_path,
                contact_points_path=contact_points_path,
                penetration_points_path=penetration_points_path,
                device=device
            )
            
            # Create object model for all objects in group
            object_model = ObjectModel(
                data_root_path=args.mesh_root,
                batch_size_each=batch_size_each,
                num_samples=1024,
                device=device
            )
            object_model.initialize(batch_object_codes)
            
            # Initialize from inference output
            hand_pose, contact_point_indices, grasp_to_object_idx = initialize_multi_object(
                hand_model, object_model, batch_grasp_data, batch_object_codes, args.n_contact, device
            )
            
            # Store starting pose for comparison
            hand_pose_st = hand_pose.detach().clone()
        
            # Setup optimizer
            optim_config = {
                'switch_possibility': args.switch_possibility,
                'starting_temperature': args.starting_temperature,
                'temperature_decay': args.temperature_decay,
                'annealing_period': args.annealing_period,
                'step_size': args.step_size,
                'stepsize_period': args.stepsize_period,
                'mu': args.mu,
                'device': device
            }
            optimizer = Annealing(hand_model, **optim_config)
            
            # Calculate initial energy
            weight_dict = dict(
                w_dis=args.w_dis,
                w_pen=args.w_pen,
                w_spen=args.w_spen,
                w_joints=args.w_joints
            )
            
            energy, E_fc, E_dis, E_pen, E_spen, E_joints = cal_energy(
                hand_model, object_model, verbose=True, **weight_dict
            )
            energy.sum().backward(retain_graph=True)
            
            # Optimization loop
            for step in tqdm(range(1, args.n_iter + 1), desc='  Optimizing', leave=False):
                s = optimizer.try_step()
                
                optimizer.zero_grad()
                new_energy, new_E_fc, new_E_dis, new_E_pen, new_E_spen, new_E_joints = cal_energy(
                    hand_model, object_model, verbose=True, **weight_dict
                )
                new_energy.sum().backward(retain_graph=True)
                
                with torch.no_grad():
                    accept, t = optimizer.accept_step(energy, new_energy)
                    
                    energy[accept] = new_energy[accept]
                    E_dis[accept] = new_E_dis[accept]
                    E_fc[accept] = new_E_fc[accept]
                    E_pen[accept] = new_E_pen[accept]
                    E_spen[accept] = new_E_spen[accept]
                    E_joints[accept] = new_E_joints[accept]
            
            # Save results for this batch - extract by object
            translation_names = ['WRJTx', 'WRJTy', 'WRJTz']
            rot_names = ['WRJRx', 'WRJRy', 'WRJRz']
            joint_names = [
                'robot0:FFJ3', 'robot0:FFJ2', 'robot0:FFJ1', 'robot0:FFJ0',
                'robot0:MFJ3', 'robot0:MFJ2', 'robot0:MFJ1', 'robot0:MFJ0',
                'robot0:RFJ3', 'robot0:RFJ2', 'robot0:RFJ1', 'robot0:RFJ0',
                'robot0:LFJ4', 'robot0:LFJ3', 'robot0:LFJ2', 'robot0:LFJ1', 'robot0:LFJ0',
                'robot0:THJ4', 'robot0:THJ3', 'robot0:THJ2', 'robot0:THJ1', 'robot0:THJ0'
            ]
            
            # Process results - organized by object
            flat_idx = 0
            for obj_idx, obj in enumerate(object_group):
                obj_code = obj['object_code']
                start_idx = batch_idx * batch_size_each
                end_idx = min(start_idx + batch_size_each, len(obj['grasp_data']))
                n_actual = max(0, end_idx - start_idx)
                
                for local_j in range(batch_size_each):
                    # Only save actual grasps, not padding
                    if local_j < n_actual:
                        idx = obj_idx * batch_size_each + local_j
                        grasp_idx_in_obj = start_idx + local_j
                        scale = obj['grasp_data'][grasp_idx_in_obj]['scale']
                        
                        # Final optimized pose
                        hand_pose_final = hand_model.hand_pose[idx].detach().cpu()
                        qpos = dict(zip(joint_names, hand_pose_final[9:].tolist()))
                        rot = robust_compute_rotation_matrix_from_ortho6d(hand_pose_final[3:9].unsqueeze(0))[0]
                        euler = transforms3d.euler.mat2euler(rot.numpy(), axes='sxyz')
                        qpos.update(dict(zip(rot_names, euler)))
                        qpos.update(dict(zip(translation_names, hand_pose_final[:3].tolist())))
                        
                        # Starting pose (from inference)
                        hand_pose_start = hand_pose_st[idx].detach().cpu()
                        qpos_st = dict(zip(joint_names, hand_pose_start[9:].tolist()))
                        rot_st = robust_compute_rotation_matrix_from_ortho6d(hand_pose_start[3:9].unsqueeze(0))[0]
                        euler_st = transforms3d.euler.mat2euler(rot_st.numpy(), axes='sxyz')
                        qpos_st.update(dict(zip(rot_names, euler_st)))
                        qpos_st.update(dict(zip(translation_names, hand_pose_start[:3].tolist())))
                        
                        result = dict(
                            scale=scale,
                            qpos=qpos,
                            qpos_st=qpos_st,
                            energy=energy[idx].item(),
                            E_fc=E_fc[idx].item(),
                            E_dis=E_dis[idx].item(),
                            E_pen=E_pen[idx].item(),
                            E_spen=E_spen[idx].item(),
                            E_joints=E_joints[idx].item(),
                        )
                        group_results[obj_code].append(result)
            
            # Clear GPU memory
            del hand_model, object_model, optimizer
            torch.cuda.empty_cache()
            
            batch_idx += 1
        
        # Save results for each object in this group
        for obj in object_group:
            obj_code = obj['object_code']
            results = group_results[obj_code]
            if len(results) > 0:
                output_path = os.path.join(args.output_dir, f"{obj_code}.npy")
                np.save(output_path, results, allow_pickle=True)
                print(f"  Saved {len(results)} optimized grasps for {obj_code}")
    
    print(f"\n{'='*70}")
    print("Post-optimization complete!")
    print(f"Results saved to: {args.output_dir}")
    print(f"{'='*70}")


def main():
    parser = argparse.ArgumentParser(description='Post-optimize grasps from CVAE inference')
    
    # Input/Output settings
    parser.add_argument('--input_dir', type=str, required=True,
                        help='Directory containing reconstructed grasps from inference (.npy files)')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='Directory to save optimized grasps')
    parser.add_argument('--mesh_root', type=str,
                        default='/home/arjun/datasets/dexgraspnet/filtered_meshes',
                        help='Path to mesh files')
    
    # Experiment settings
    parser.add_argument('--seed', default=1, type=int)
    parser.add_argument('--n_objects', default=1, type=int,
                        help='Number of objects to process simultaneously (increase to use more VRAM)')
    parser.add_argument('--batch_size', default=128, type=int,
                        help='Maximum batch size per object (will process in chunks if more grasps)')
    parser.add_argument('--n_iter', default=6000, type=int,
                        help='Number of optimization iterations')
    parser.add_argument('--n_contact', default=4, type=int)
    
    # Hyper parameters (same as original DexGraspNet)
    parser.add_argument('--switch_possibility', default=0.5, type=float)
    parser.add_argument('--mu', default=0.98, type=float)
    parser.add_argument('--step_size', default=0.005, type=float)
    parser.add_argument('--stepsize_period', default=50, type=int)
    parser.add_argument('--starting_temperature', default=18, type=float)
    parser.add_argument('--annealing_period', default=30, type=int)
    parser.add_argument('--temperature_decay', default=0.95, type=float)
    parser.add_argument('--w_dis', default=100.0, type=float)
    parser.add_argument('--w_pen', default=100.0, type=float)
    parser.add_argument('--w_spen', default=10.0, type=float)
    parser.add_argument('--w_joints', default=1.0, type=float)
    
    # Energy thresholds
    parser.add_argument('--thres_fc', default=0.3, type=float)
    parser.add_argument('--thres_dis', default=0.005, type=float)
    parser.add_argument('--thres_pen', default=0.001, type=float)
    
    args = parser.parse_args()
    
    # Print configuration
    print("=" * 70)
    print("Post-Optimization for CVAE Reconstructed Grasps")
    print("=" * 70)
    print(f"\nConfiguration:")
    print(f"  Input directory: {args.input_dir}")
    print(f"  Output directory: {args.output_dir}")
    print(f"  Mesh root: {args.mesh_root}")
    print(f"  Objects at a time: {args.n_objects}")
    print(f"  Batch size per object: {args.batch_size}")
    print(f"  Iterations: {args.n_iter}")
    print("=" * 70)
    
    post_optimize(args)


if __name__ == '__main__':
    main()
